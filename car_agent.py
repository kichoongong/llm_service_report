"""
LangGraph ê¸°ë°˜ ìë™ì°¨ ì—ì´ì „íŠ¸
"""

import json
import re
from datetime import datetime
from typing import Dict, Any, Literal
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma

from main import CarAgentState
from corrective_rag import LangGraphCorrectiveRAG


# LangGraph ë…¸ë“œë“¤ êµ¬í˜„
def route_input_node(state: CarAgentState) -> CarAgentState:
    """ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤."""
    print(f"ğŸ” ì…ë ¥ ë¶„ì„ ì¤‘: {state['user_input']}")

    try:
        # ë¼ìš°íŒ… í”„ë¡¬í”„íŠ¸
        routing_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ìë™ì°¨ ì—ì´ì „íŠ¸ì˜ ë¼ìš°íŒ… ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

1. **car_control**: ìë™ì°¨ ê¸°ëŠ¥ ì œì–´ ìš”ì²­
   - ì°½ë¬¸ ì—´ê¸°/ë‹«ê¸°, ì¡°ëª… ì¼œê¸°/ë„ê¸°, ì˜¤ë””ì˜¤ ì¡°ì ˆ, ì—ì–´ì»¨/íˆí„° ì¡°ì ˆ ë“±
   - í‚¤ì›Œë“œ: "ì—´ì–´", "ë‹«ì•„", "ì¼œ", "êº¼", "ì¡°ì ˆ", "ì„¤ì •", "ì˜¬ë ¤", "ë‚´ë ¤"

2. **car_manual**: ìë™ì°¨ ë§¤ë‰´ì–¼ ê²€ìƒ‰ ìš”ì²­
   - ê¸°ëŠ¥ ì„¤ëª…, ì‚¬ìš©ë²•, ì£¼ì˜ì‚¬í•­, ë¬¸ì œ í•´ê²° ë“±
   - í‚¤ì›Œë“œ: "ë°©ë²•", "ì„¤ëª…", "ì–´ë–»ê²Œ", "ì™œ", "ì£¼ì˜", "ë¬¸ì œ", "í•´ê²°"

3. **fallback**: ì¼ë°˜ì ì¸ ëŒ€í™”
   - ì¸ì‚¬, ê°ì‚¬, ì¼ìƒ ëŒ€í™”, ìë™ì°¨ì™€ ë¬´ê´€í•œ ì§ˆë¬¸

ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "route_type": "car_control" | "car_manual" | "fallback",
    "confidence": 0.0-1.0,
    "reasoning": "ë¶„ë¥˜ ì´ìœ "
}}""",
                ),
                ("human", "{user_input}"),
            ]
        )

        # ë¼ìš°íŒ… ì‹¤í–‰
        routing_chain = (
            routing_prompt
            | ChatOpenAI(model="gpt-4o-mini", temperature=0)
            | StrOutputParser()
        )
        routing_result = routing_chain.invoke({"user_input": state["user_input"]})

        # JSON íŒŒì‹±
        routing_data = json.loads(routing_result)

        print(
            f"ğŸ“Š ë¼ìš°íŒ… ê²°ê³¼: {routing_data['route_type']} (ì‹ ë¢°ë„: {routing_data['confidence']:.2f})"
        )

        return {
            **state,
            "route_type": routing_data["route_type"],
            "confidence": routing_data["confidence"],
            "messages": state.get("messages", [])
            + [HumanMessage(content=f"ë¼ìš°íŒ…: {routing_data['route_type']}")],
        }

    except Exception as e:
        print(f"âŒ ë¼ìš°íŒ… ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "route_type": "fallback",
            "confidence": 0.5,
            "error_message": str(e),
        }


def car_control_node(state: CarAgentState) -> CarAgentState:
    """ìë™ì°¨ ì œì–´ ë…¸ë“œ"""
    print("ğŸš— ìë™ì°¨ ì œì–´ ì²˜ë¦¬ ì¤‘...")

    try:
        # ìë™ì°¨ ì œì–´ í”„ë¡¬í”„íŠ¸
        control_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ìë™ì°¨ ì œì–´ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ êµ¬ì²´ì ì¸ ì œì–´ ëª…ë ¹ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì§€ì›í•˜ëŠ” ì œì–´ ê¸°ëŠ¥:
1. ì°½ë¬¸ ì œì–´: ì—´ê¸°/ë‹«ê¸°, ì˜¬ë¦¬ê¸°/ë‚´ë¦¬ê¸°
2. ì¡°ëª… ì œì–´: ì „ì¡°ë“±, í›„ë¯¸ë“±, ì‹¤ë‚´ë“± ì¼œê¸°/ë„ê¸°
3. ì˜¤ë””ì˜¤ ì œì–´: ë³¼ë¥¨ ì¡°ì ˆ, ìŒì› ë³€ê²½
4. ì˜¨ë„ ì œì–´: ì—ì–´ì»¨/íˆí„° ì˜¨ë„ ì¡°ì ˆ
5. ì‹œíŠ¸ ì œì–´: ì‹œíŠ¸ ìœ„ì¹˜ ì¡°ì ˆ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
{{
    "action_type": "window",
    "action": "open",
    "target": "all",
    "value": null,
    "response": "ì°½ë¬¸ì„ ì—´ì—ˆìŠµë‹ˆë‹¤."
}}""",
                ),
                ("human", "{user_input}"),
            ]
        )

        # ì œì–´ ëª…ë ¹ ìƒì„±
        control_chain = (
            control_prompt
            | ChatOpenAI(model="gpt-4o-mini", temperature=0)
            | StrOutputParser()
        )
        control_result = control_chain.invoke({"user_input": state["user_input"]})

        print(f"ğŸ” LLM ì›ë³¸ ì‘ë‹µ: '{control_result}'")

        # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not control_result or control_result.strip() == "":
            print("âš ï¸ LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            control_data = {
                "action_type": "unknown",
                "action": "unknown",
                "target": "all",
                "value": None,
                "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. '{state['user_input']}' ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            }
        else:
            # JSON ì¶”ì¶œ ì‹œë„
            control_data = None

            # ë°©ë²• 1: ì§ì ‘ JSON íŒŒì‹±
            try:
                control_data = json.loads(control_result.strip())
                print("âœ… ì§ì ‘ JSON íŒŒì‹± ì„±ê³µ")
            except json.JSONDecodeError:
                print("âŒ ì§ì ‘ JSON íŒŒì‹± ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ JSON ì¶”ì¶œ ì‹œë„")

                # ë°©ë²• 2: ì •ê·œì‹ìœ¼ë¡œ JSON ì¶”ì¶œ
                json_match = re.search(r"\{.*\}", control_result, re.DOTALL)
                if json_match:
                    try:
                        json_str = json_match.group()
                        control_data = json.loads(json_str)
                        print("âœ… ì •ê·œì‹ JSON ì¶”ì¶œ ì„±ê³µ")
                    except json.JSONDecodeError:
                        print("âŒ ì •ê·œì‹ JSON ì¶”ì¶œë„ ì‹¤íŒ¨")

            # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            if control_data is None:
                print("âš ï¸ ëª¨ë“  JSON íŒŒì‹± ë°©ë²• ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
                control_data = {
                    "action_type": "unknown",
                    "action": "unknown",
                    "target": "all",
                    "value": None,
                    "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. '{state['user_input']}' ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }

        print(f"ğŸ“Š ìµœì¢… íŒŒì‹±ëœ ë°ì´í„°: {control_data}")

        # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
        if "response" not in control_data or not control_data["response"]:
            control_data["response"] = f"'{state['user_input']}' ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤."
            print("âš ï¸ response í•„ë“œê°€ ì—†ì–´ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")

        # ì‹œë®¬ë ˆì´ì…˜ëœ ì œì–´ ì‹¤í–‰
        control_action = f"{control_data.get('action_type', 'unknown')}_{control_data.get('action', 'unknown')}_{control_data.get('target', 'all')}"
        if control_data.get("value"):
            control_action += f"_{control_data['value']}"

        print(f"ğŸ® ì œì–´ ëª…ë ¹ ì‹¤í–‰: {control_action}")
        print(f"ğŸ’¬ ì‚¬ìš©ì ì‘ë‹µ: {control_data['response']}")

        return {
            **state,
            "control_action": control_action,
            "control_result": control_data["response"],
            "final_response": control_data["response"],
            "messages": state.get("messages", [])
            + [AIMessage(content=f"ìë™ì°¨ ì œì–´: {control_data['response']}")],
        }

    except Exception as e:
        print(f"âŒ ìë™ì°¨ ì œì–´ ì¤‘ ì˜¤ë¥˜: {e}")
        error_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ìë™ì°¨ ì œì–´ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return {
            **state,
            "control_action": "error",
            "control_result": error_response,
            "final_response": error_response,
            "error_message": str(e),
        }


def car_manual_node(state: CarAgentState) -> CarAgentState:
    """ìë™ì°¨ ë§¤ë‰´ì–¼ ê²€ìƒ‰ ë…¸ë“œ"""
    print("ğŸ“– ìë™ì°¨ ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘...")

    try:
        # Corrective RAG Agent ìƒì„± (ì„ì‹œ)
        corrective_rag = LangGraphCorrectiveRAG(
            state.get("vector_db"),
            ChatOpenAI(model="gpt-4o-mini", temperature=0),
            max_iterations=3,
        )

        # Corrective RAGë¡œ ì²˜ë¦¬
        result = corrective_rag.process_question(state["user_input"])

        print(f"ğŸ“ Corrective RAG ë‹µë³€ ìƒì„± ì™„ë£Œ")

        return {
            **state,
            "search_query": state["user_input"],
            "search_results": [],  # Corrective RAG ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
            "manual_answer": result["answer"],
            "final_response": result["answer"],
            "messages": state.get("messages", [])
            + [AIMessage(content=f"ë§¤ë‰´ì–¼ ê²€ìƒ‰: {result['answer'][:100]}...")],
        }

    except Exception as e:
        print(f"âŒ ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "search_query": state["user_input"],
            "search_results": [],
            "manual_answer": "ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "final_response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "error_message": str(e),
        }


def fallback_node(state: CarAgentState) -> CarAgentState:
    """ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” í´ë°± ë…¸ë“œ"""
    print("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ ì¤‘...")

    try:
        # ì¼ë°˜ ëŒ€í™” í”„ë¡¬í”„íŠ¸
        fallback_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ì¹œê·¼í•œ ìë™ì°¨ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì–´ì£¼ì„¸ìš”.

ëŒ€í™” ìŠ¤íƒ€ì¼:
1. ì¹œê·¼í•˜ê³  ì •ì¤‘í•œ í†¤
2. ìë™ì°¨ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ë„ì›€ì„ ì œê³µ
3. ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë©´ ì ì ˆíˆ ì‘ë‹µ
4. í•„ìš”ì‹œ ìë™ì°¨ ê¸°ëŠ¥ì„ ì•ˆë‚´
5. ì‚¬ìš©ìì˜ ê°ì •ì„ ì´í•´í•˜ê³  ê³µê°

ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:""",
                ),
                ("human", "{user_input}"),
            ]
        )

        fallback_chain = (
            fallback_prompt
            | ChatOpenAI(model="gpt-4o-mini", temperature=0)
            | StrOutputParser()
        )
        general_response = fallback_chain.invoke({"user_input": state["user_input"]})

        print(f"ğŸ’­ ì¼ë°˜ ëŒ€í™” ì‘ë‹µ ìƒì„± ì™„ë£Œ")

        return {
            **state,
            "general_response": general_response,
            "final_response": general_response,
            "messages": state.get("messages", [])
            + [AIMessage(content=f"ì¼ë°˜ ëŒ€í™”: {general_response}")],
        }

    except Exception as e:
        print(f"âŒ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "general_response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "final_response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "error_message": str(e),
        }


def route_to_node(
    state: CarAgentState,
) -> Literal["car_control", "car_manual", "fallback"]:
    """ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë…¸ë“œë¡œ ë¶„ê¸°í•©ë‹ˆë‹¤."""
    route_type = state.get("route_type", "fallback")
    confidence = state.get("confidence", 0.0)

    print(f"ğŸ”„ ë¼ìš°íŒ…: {route_type} (ì‹ ë¢°ë„: {confidence:.2f})")

    # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í´ë°±ìœ¼ë¡œ ì²˜ë¦¬
    if confidence < 0.3:
        return "fallback"

    return route_type


class LangGraphCarAgent:
    """LangGraph ê¸°ë°˜ ìë™ì°¨ ì—ì´ì „íŠ¸"""

    def __init__(self, vector_db: Chroma, llm: ChatOpenAI):
        self.vector_db = vector_db
        self.llm = llm
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """LangGraph ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""

        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(CarAgentState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("route_input", route_input_node)
        workflow.add_node("car_control", car_control_node)
        workflow.add_node("car_manual", car_manual_node)
        workflow.add_node("fallback", fallback_node)

        # ì‹œì‘ì  ì„¤ì •
        workflow.set_entry_point("route_input")

        # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "route_input",
            route_to_node,
            {
                "car_control": "car_control",
                "car_manual": "car_manual",
                "fallback": "fallback",
            },
        )

        # ëª¨ë“  ë…¸ë“œì—ì„œ ì¢…ë£Œ
        workflow.add_edge("car_control", END)
        workflow.add_edge("car_manual", END)
        workflow.add_edge("fallback", END)

        return workflow.compile()

    def process_input(self, user_input: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        print(f"ğŸ¤– LangGraph ìë™ì°¨ ì—ì´ì „íŠ¸ê°€ ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤: '{user_input}'")

        start_time = datetime.now()

        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "user_input": user_input,
            "messages": [HumanMessage(content=user_input)],
            "route_type": None,
            "confidence": 0.0,
            "control_action": None,
            "control_result": None,
            "search_query": None,
            "search_results": [],
            "manual_answer": None,
            "general_response": None,
            "final_response": "",
            "timestamp": datetime.now().isoformat(),
            "processing_time": 0.0,
            "error_message": None,
            "vector_db": self.vector_db,  # ë²¡í„° DB ì „ë‹¬
        }

        # ê·¸ë˜í”„ ì‹¤í–‰
        try:
            final_state = self.graph.invoke(initial_state)

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            # ê²°ê³¼ ì •ë¦¬
            result = {
                "user_input": user_input,
                "route_type": final_state.get("route_type", "fallback"),
                "confidence": final_state.get("confidence", 0.0),
                "final_response": final_state.get("final_response", ""),
                "control_action": final_state.get("control_action"),
                "search_query": final_state.get("search_query"),
                "manual_answer": final_state.get("manual_answer"),
                "general_response": final_state.get("general_response"),
                "processing_time": processing_time,
                "timestamp": final_state.get("timestamp"),
                "error_message": final_state.get("error_message"),
            }

            print(
                f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ - ë¼ìš°íŒ…: {result['route_type']}, ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ"
            )

            return result

        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "user_input": user_input,
                "route_type": "error",
                "confidence": 0.0,
                "final_response": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "processing_time": (datetime.now() - start_time).total_seconds(),
                "timestamp": datetime.now().isoformat(),
                "error_message": str(e),
            }
