#!/usr/bin/env python3
"""
í–¥ìƒëœ PDF ë¬¸ì„œ êµ¬ì¡°í™” ì²˜ë¦¬ê¸°
ìë™ì°¨ ë§¤ë‰´ì–¼ì— ìµœì í™”ëœ ë¬¸ì„œ êµ¬ì¡°í™” ë° ì„ë² ë”© ë²¡í„° ì €ì¥
"""
import os
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from dotenv import load_dotenv

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


@dataclass
class DocumentSection:
    """ë¬¸ì„œ ì„¹ì…˜ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""

    title: str
    content: str
    page_number: int
    section_type: str  # 'header', 'content', 'table', 'list', 'warning', 'note'
    level: int  # í—¤ë” ë ˆë²¨ (1, 2, 3, ...)
    parent_section: Optional[str] = None
    metadata: Dict[str, Any] = None


class EnhancedPDFProcessor:
    """í–¥ìƒëœ PDF ë¬¸ì„œ êµ¬ì¡°í™” ì²˜ë¦¬ê¸°"""

    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.pages = []
        self.sections = []
        self.structure_info = {}

        # ìë™ì°¨ ë§¤ë‰´ì–¼ íŠ¹í™” í‚¤ì›Œë“œ íŒ¨í„´
        self.car_keywords = {
            "engine": ["ì—”ì§„", "ëª¨í„°", "ì‹œë™", "ê°€ì†", "ì—°ë£Œ", "ì˜¤ì¼", "ëƒ‰ê°ìˆ˜"],
            "safety": ["ì•ˆì „", "ì‚¬ê³ ", "ì—ì–´ë°±", "ë²¨íŠ¸", "ë¸Œë ˆì´í¬", "ABS", "ESP"],
            "comfort": ["í¸ì˜", "ì‹œíŠ¸", "ì—ì–´ì»¨", "íˆí„°", "ì˜¨ë„", "ì¡°ì ˆ"],
            "navigation": ["ë‚´ë¹„ê²Œì´ì…˜", "GPS", "ê¸¸ì°¾ê¸°", "ì§€ë„", "ê²½ë¡œ", "ëª©ì ì§€"],
            "audio": ["ì˜¤ë””ì˜¤", "ìŒì•…", "ë¼ë””ì˜¤", "ìŠ¤í”¼ì»¤", "ë³¼ë¥¨", "ìŒì§ˆ"],
            "lighting": ["ì¡°ëª…", "ë¼ì´íŠ¸", "ë¶ˆë¹›", "ì „ì¡°ë“±", "í›„ë¯¸ë“±", "ì‹¤ë‚´ë“±"],
            "door": ["ë¬¸", "ì°½ë¬¸", "ì ê¸ˆ", "ì—´ê¸°", "ë‹«ê¸°", "ìë™ë¬¸"],
            "maintenance": ["ì ê²€", "ì •ë¹„", "êµì²´", "ìˆ˜ë¦¬", "ì„œë¹„ìŠ¤", "ì²´í¬"],
            "warning": ["ì£¼ì˜", "ê²½ê³ ", "ìœ„í—˜", "ì£¼ì˜ì‚¬í•­", "ì£¼ì˜í•˜ì„¸ìš”"],
            "information": ["ì •ë³´", "ì°¸ê³ ", "ì„¤ëª…", "ì•ˆë‚´", "ë„ì›€ë§"],
        }

        # ì„¹ì…˜ í—¤ë” íŒ¨í„´ (í•œêµ­ì–´ ìë™ì°¨ ë§¤ë‰´ì–¼ì— íŠ¹í™”)
        self.header_patterns = [
            r"^[0-9]+\.[0-9]*\s+[ê°€-í£\s]+$",  # 1.1 ì œëª©
            r"^[0-9]+\s+[ê°€-í£\s]+$",  # 1 ì œëª©
            r"^[A-Z][A-Z\s]+$",  # ëŒ€ë¬¸ì ì œëª©
            r"^[ê°€-í£]+[ê°€-í£\s]*:$",  # í•œê¸€ ì œëª©:
            r"^[ê°€-í£]+[ê°€-í£\s]*\s*\([ê°€-í£\s]+\)$",  # í•œê¸€ ì œëª© (ë¶€ì œëª©)
        ]

        # í‘œ ê°ì§€ íŒ¨í„´
        self.table_patterns = [
            r"^\s*\|.*\|.*$",  # íŒŒì´í”„ êµ¬ë¶„ì
            r"^\s*[ê°€-í£\s]+\s+[ê°€-í£\s]+\s+[ê°€-í£\s]+$",  # 3ì—´ ì´ìƒ ì •ë ¬
            r"^\s*[ê°€-í£]+.*\s+[0-9]+.*\s+[ê°€-í£]+.*$",  # ë°ì´í„° í–‰
        ]

        # ê²½ê³ /ì£¼ì˜ì‚¬í•­ íŒ¨í„´
        self.warning_patterns = [
            r"ì£¼ì˜[ì‚¬í•­]*[:ï¼š]",
            r"ê²½ê³ [:ï¼š]",
            r"ìœ„í—˜[:ï¼š]",
            r"ì£¼ì˜í•˜ì„¸ìš”",
            r"ì£¼ì˜[!ï¼]",
            r"ê²½ê³ [!ï¼]",
        ]

    def load_pdf(self) -> List[Document]:
        """PDF íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘: {self.pdf_path}")
        loader = PyMuPDFLoader(self.pdf_path)
        self.pages = loader.load()
        print(f"âœ… {len(self.pages)}ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        return self.pages

    def detect_section_type(self, content: str) -> str:
        """í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì„¹ì…˜ íƒ€ì…ì„ ê°ì§€í•©ë‹ˆë‹¤."""
        content_lower = content.lower()

        # ê²½ê³ /ì£¼ì˜ì‚¬í•­ ê°ì§€
        for pattern in self.warning_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return "warning"

        # í‘œ ê°ì§€
        for pattern in self.table_patterns:
            if re.search(pattern, content, re.MULTILINE):
                return "table"

        # ë¦¬ìŠ¤íŠ¸ ê°ì§€
        if re.search(r"^\s*[â€¢Â·â–ªâ–«-]\s", content, re.MULTILINE):
            return "list"

        # í—¤ë” ê°ì§€
        for pattern in self.header_patterns:
            if re.match(pattern, content.strip()):
                return "header"

        return "content"

    def extract_sections_from_page(
        self, page: Document, page_num: int
    ) -> List[DocumentSection]:
        """í˜ì´ì§€ì—ì„œ ì„¹ì…˜ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content = page.page_content
        sections = []

        # í˜ì´ì§€ë¥¼ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• 
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]

        current_section = None
        section_level = 1

        for para in paragraphs:
            if not para:
                continue

            section_type = self.detect_section_type(para)

            # í—¤ë”ì¸ ê²½ìš°
            if section_type == "header":
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section:
                    sections.append(current_section)

                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = DocumentSection(
                    title=para,
                    content="",
                    page_number=page_num,
                    section_type=section_type,
                    level=section_level,
                    metadata=page.metadata.copy(),
                )
                section_level += 1

            else:
                # ë‚´ìš©ì„ í˜„ì¬ ì„¹ì…˜ì— ì¶”ê°€
                if current_section:
                    if current_section.content:
                        current_section.content += "\n\n" + para
                    else:
                        current_section.content = para
                    current_section.section_type = section_type
                else:
                    # í—¤ë”ê°€ ì—†ëŠ” ë‚´ìš©ì€ ë…ë¦½ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                    current_section = DocumentSection(
                        title=f"í˜ì´ì§€ {page_num} ë‚´ìš©",
                        content=para,
                        page_number=page_num,
                        section_type=section_type,
                        level=1,
                        metadata=page.metadata.copy(),
                    )

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section:
            sections.append(current_section)

        return sections

    def analyze_document_structure(self) -> Dict[str, Any]:
        """ë¬¸ì„œì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì¤‘...")

        all_sections = []
        structure_stats = {
            "total_pages": len(self.pages),
            "total_sections": 0,
            "section_types": {},
            "page_sections": {},
            "hierarchy": {},
            "tables": [],
            "warnings": [],
            "lists": [],
        }

        for i, page in enumerate(self.pages):
            page_sections = self.extract_sections_from_page(page, i + 1)
            all_sections.extend(page_sections)
            structure_stats["page_sections"][i + 1] = len(page_sections)

            for section in page_sections:
                # ì„¹ì…˜ íƒ€ì…ë³„ í†µê³„
                section_type = section.section_type
                if section_type not in structure_stats["section_types"]:
                    structure_stats["section_types"][section_type] = 0
                structure_stats["section_types"][section_type] += 1

                # íŠ¹ìˆ˜ ì„¹ì…˜ ìˆ˜ì§‘
                if section_type == "table":
                    structure_stats["tables"].append(
                        {
                            "page": i + 1,
                            "title": section.title,
                            "content": (
                                section.content[:100] + "..."
                                if len(section.content) > 100
                                else section.content
                            ),
                        }
                    )
                elif section_type == "warning":
                    structure_stats["warnings"].append(
                        {
                            "page": i + 1,
                            "title": section.title,
                            "content": (
                                section.content[:100] + "..."
                                if len(section.content) > 100
                                else section.content
                            ),
                        }
                    )
                elif section_type == "list":
                    structure_stats["lists"].append(
                        {
                            "page": i + 1,
                            "title": section.title,
                            "content": (
                                section.content[:100] + "..."
                                if len(section.content) > 100
                                else section.content
                            ),
                        }
                    )

        structure_stats["total_sections"] = len(all_sections)
        self.sections = all_sections
        self.structure_info = structure_stats

        print(f"ğŸ“Š ì´ ì„¹ì…˜ ìˆ˜: {structure_stats['total_sections']}")
        print(f"ğŸ“Š ì„¹ì…˜ íƒ€ì…ë³„ ë¶„í¬: {structure_stats['section_types']}")
        print(f"ğŸ“Š í‘œ ê°œìˆ˜: {len(structure_stats['tables'])}")
        print(f"ğŸ“Š ê²½ê³ ì‚¬í•­ ê°œìˆ˜: {len(structure_stats['warnings'])}")

        return structure_stats

    def categorize_content(self, content: str) -> List[str]:
        """ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        categories = []
        content_lower = content.lower()

        for category, keywords in self.car_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                categories.append(category)

        return categories if categories else ["general"]

    def create_enhanced_chunks(self) -> List[Document]:
        """êµ¬ì¡°í™”ëœ ì„¹ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("âœ‚ï¸ êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„± ì¤‘...")

        chunks = []
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # ì„¹ì…˜ ê¸°ë°˜ìœ¼ë¡œ í¬ê¸° ì¦ê°€
            chunk_overlap=100,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        for i, section in enumerate(self.sections):
            # ì„¹ì…˜ë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
            categories = self.categorize_content(section.content)

            # ì„¹ì…˜ì´ ë„ˆë¬´ í¬ë©´ ë¶„í• 
            if len(section.content) > 1000:
                section_chunks = text_splitter.split_text(section.content)
                for j, chunk_text in enumerate(section_chunks):
                    chunk_metadata = {
                        **section.metadata,
                        "section_title": section.title,
                        "section_type": section.section_type,
                        "section_level": section.level,
                        "page_number": section.page_number,
                        "categories": ", ".join(categories),
                        "chunk_index": j,
                        "total_chunks_in_section": len(section_chunks),
                        "is_structured": True,
                        "document_type": "car_manual",
                        "language": "ko",
                        "chunk_length": len(chunk_text),
                    }

                    chunk = Document(page_content=chunk_text, metadata=chunk_metadata)
                    chunks.append(chunk)
            else:
                # ì„¹ì…˜ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬
                chunk_metadata = {
                    **section.metadata,
                    "section_title": section.title,
                    "section_type": section.section_type,
                    "section_level": section.level,
                    "page_number": section.page_number,
                    "categories": ", ".join(categories),
                    "chunk_index": 0,
                    "total_chunks_in_section": 1,
                    "is_structured": True,
                    "document_type": "car_manual",
                    "language": "ko",
                    "chunk_length": len(section.content),
                }

                chunk = Document(page_content=section.content, metadata=chunk_metadata)
                chunks.append(chunk)

        print(f"âœ… {len(chunks)}ê°œ êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunks

    def create_vector_database(
        self, chunks: List[Document], collection_name: str = "car_manual"
    ) -> Chroma:
        """êµ¬ì¡°í™”ëœ ì²­í¬ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        print("ğŸ§  ì„ë² ë”© ë²¡í„° ìƒì„± ë° ì €ì¥ ì¤‘...")

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ ìƒì„±
        persist_directory = f"./vector_db/{collection_name}"
        if os.path.exists(persist_directory):
            import shutil

            shutil.rmtree(persist_directory)

        os.makedirs(persist_directory, exist_ok=True)

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë²¡í„° DB ìƒì„±
        batch_size = 50
        vector_db = None

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            print(
                f"ë°°ì¹˜ {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘..."
            )

            if vector_db is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ì´ˆê¸° DB ìƒì„±
                vector_db = Chroma.from_documents(
                    documents=batch,
                    embedding=embeddings,
                    persist_directory=persist_directory,
                    collection_name=collection_name,
                )
            else:
                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
                vector_db.add_documents(batch)

        print(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ: {persist_directory}")
        return vector_db

    def save_structure_analysis(self, output_path: str = "structure_analysis.json"):
        """êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        analysis_data = {
            "document_info": {
                "file_path": self.pdf_path,
                "total_pages": self.structure_info.get("total_pages", 0),
                "total_sections": self.structure_info.get("total_sections", 0),
            },
            "section_analysis": self.structure_info,
            "sections": [
                {
                    "title": section.title,
                    "type": section.section_type,
                    "level": section.level,
                    "page": section.page_number,
                    "content_length": len(section.content),
                    "content_preview": (
                        section.content[:200] + "..."
                        if len(section.content) > 200
                        else section.content
                    ),
                }
                for section in self.sections
            ],
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")

    def process_document(self) -> Tuple[Chroma, Dict[str, Any]]:
        """ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸš— ìë™ì°¨ ë§¤ë‰´ì–¼ PDF êµ¬ì¡°í™” ì²˜ë¦¬ ì‹œì‘...")

        # 1. PDF ë¡œë“œ
        self.load_pdf()

        # 2. êµ¬ì¡° ë¶„ì„
        structure_info = self.analyze_document_structure()

        # 3. êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„±
        chunks = self.create_enhanced_chunks()

        # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        vector_db = self.create_vector_database(chunks)

        # 5. êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.save_structure_analysis()

        print("ğŸ‰ ë¬¸ì„œ êµ¬ì¡°í™” ì²˜ë¦¬ ì™„ë£Œ!")
        return vector_db, structure_info


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pdf_file = "data/RS4_2025_ko_KR.pdf"

    if not Path(pdf_file).exists():
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}")
        return

    # í–¥ìƒëœ PDF ì²˜ë¦¬ê¸° ì‹¤í–‰
    processor = EnhancedPDFProcessor(pdf_file)
    vector_db, structure_info = processor.process_document()

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” êµ¬ì¡°í™”ëœ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    test_queries = [
        "ì—”ì§„ ì‹œë™ ë°©ë²•",
        "ì•ˆì „ë²¨íŠ¸ ì‚¬ìš©ë²•",
        "ì—ì–´ì»¨ ì¡°ì‘",
        "ì£¼ì˜ì‚¬í•­",
        "í‘œ ì •ë³´",
    ]

    for query in test_queries:
        print(f"\nê²€ìƒ‰ì–´: '{query}'")
        results = vector_db.similarity_search(query, k=2)
        for i, result in enumerate(results):
            print(f"  {i+1}. {result.metadata.get('section_title', 'N/A')}")
            print(f"     íƒ€ì…: {result.metadata.get('section_type', 'N/A')}")
            print(f"     í˜ì´ì§€: {result.metadata.get('page_number', 'N/A')}")
            print(f"     ë‚´ìš©: {result.page_content[:100]}...")


if __name__ == "__main__":
    main()
